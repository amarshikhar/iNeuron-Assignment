{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bound-arthritis",
   "metadata": {},
   "source": [
    "Q1. What is the function of a summation junction of a neuron? What is threshold activation\n",
    "function?\n",
    "\n",
    "Ans:A summation junction for the input signals is weighted by the respective synaptic weight. Because it is a linear combiner or adder of the weighted input signals, the output of the summation junction can be expressed as follows:y=(w_i.x_i)_i=1 to n\n",
    "\n",
    "A threshold activation function (or simply the activation function, also known as squashing function) results in an output signal only when an input signal exceeding a specific threshold value comes as an input. It is similar in behaviour to the biological neuron which transmits the signal only when the total input signal meets the firing threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-spending",
   "metadata": {},
   "source": [
    "Q2. What is a step function? What is the difference of step function with threshold function?\n",
    "\n",
    "Ans: Threshold/step Function: It is a commonly used activation function. As depicted in the diagram, it gives 1 as output of the input is either 0 or positive. If the input is negative, it gives 0 as output. \n",
    "\n",
    "The threshold function is almost like the step function, with the only difference being a fact that theta    is used as a threshold value instead of 0.\n",
    "\n",
    "Q3. Explain the McCulloch–Pitts model of neuron.\n",
    "\n",
    "Ans: The McCulloch-Pitts neural model, which was the earliest ANN model, has only two types of inputs — Excitatory and Inhibitory. The excitatory inputs have weights of positive magnitude and the inhibitory weights have weights of negative magnitude. The inputs of the McCulloch-Pitts neuron could be either 0 or 1. It has a threshold function as an activation function. So, the output signal yout is 1 if the input ysum is greater than or equal to a given threshold value, else 0. \n",
    "\n",
    "Q4. Explain the ADALINE network model.\n",
    "\n",
    "Ans: Adline stands for adaptive linear neuron. It makes use of linear activation function, and it uses the delta rule for training to minimize the mean squared errors between the actual output and the desired target output. The weights and bias are adjustable. Here, we perform 10 epochs of training and calculate total mean error in each case, the total mean error decreases after certain epochs and later becomes nearly constant.\n",
    "\n",
    "Q5. What is the constraint of a simple perceptron? Why it may fail with a real-world data set?\n",
    "\n",
    "Ans:Perceptron networks have several limitations. First, the output values of a perceptron can take on only one of two values (0 or 1) due to the hard-limit transfer function. Second, perceptrons can only classify linearly separable sets of vectors. If a straight line or a plane can be drawn to separate the input vectors into their correct categories, the input vectors are linearly separable. If the vectors are not linearly separable, learning will never reach a point where all vectors are classified properly. Note, however, that it has been proven that if the vectors are linearly separable, perceptrons trained adaptively will always find a solution in finite time. \n",
    "\n",
    "Q6. What is linearly inseparable problem? What is the role of the hidden layer?\n",
    "\n",
    "Ans: A set of input vectors (or a training set) will be said to be linearly non-separable if no hyperplane exists such that each vector lies on the pre-assigned side of the hyperplane.\n",
    "\n",
    "In neural networks, a hidden layer is located between the input and output of the algorithm, in which the function applies weights to the inputs and directs them through an activation function as the output. In short, the hidden layers perform nonlinear transformations of the inputs entered into the network.\n",
    "\n",
    "Q7. Explain XOR problem in case of a simple perceptron.\n",
    "\n",
    "Ans: The XOr problem is that we need to build a Neural Network (a perceptron in our case) to produce the truth table related to the XOr logical operator. This is a binary classification problem. Hence, supervised learning is a better way to solve it. In this case, we will be using perceptrons. Uni layered perceptrons can only work with linearly separable data.\n",
    "\n",
    "To solve this problem, we add an extra layer to our vanilla perceptron, i.e., we create a Multi Layered Perceptron (or MLP). We call this extra layer as the Hidden layer. To build a perceptron, we first need to understand that the XOr gate can be written as a combination of AND gates, NOT gates and OR gates in the following way:\n",
    "a XOr b = (a AND NOT b)OR(bAND NOTa)\n",
    "\n",
    "Q8. Design a multi-layer perceptron to implement A XOR B.\n",
    "\n",
    "Ans:To solve this problem, we add an extra layer to our vanilla perceptron, i.e., we create a Multi Layered Perceptron (or MLP). We call this extra layer as the Hidden layer(h1 and h2 node). To build a perceptron, we first need to understand that the XOr gate can be written as a combination of AND gates, NOT gates and OR gates in the following way:\n",
    "a XOr b = (a AND NOT b)OR(bAND NOTa)\n",
    "\n",
    "We need to observe that our inputs are 0s and 1s. To make it a XOr gate, we will make the h1 node to perform the (x2 AND NOT x1) operation, the h2 node to perform (x1 AND NOT x2) operation and the y node to perform (h1 OR h2) operation. \n",
    "\n",
    "Q9. Explain the single-layer feed forward architecture of ANN.\n",
    "\n",
    "Ans: In this type of network, we have only two layers input layer and output layer but the input layer does not count because no computation is performed in this layer. The output layer is formed when different weights are applied on input nodes and the cumulative effect per node is taken.\n",
    "\n",
    "Q10. Explain the competitive network architecture of ANN.\n",
    "\n",
    "Ans:In classification and prediction problems, we are provided with training sets with desired outputs, so backpropagation together with feed-forward networks are useful in modeling the input-output relationship. However, sometimes we have to analyze raw data of which we have no prior knowledge. The only possible way is to find out special features of the data and arrange the data in clusters so that elements that are similar to each other are grouped together. Such a process can be readily performed using simple competitive networks.\n",
    "\n",
    "Simple competitive networks are composed of two networks: the Hemming net and the Maxnet. Each of them specializes in a different function:\n",
    "\n",
    "1.\tThe Hemming net measures how much the input vector resembles the weight vector of each perceptron.\n",
    "2.\tThe maxnet finds the perceptron with the maximum value.\n",
    "\n",
    "Q11. Consider a multi-layer feed forward neural network. Enumerate and explain steps in the\n",
    "backpropagation algorithm used to train the network.\n",
    "\n",
    "Ans:Now, what we did here:\n",
    "\n",
    ">We first initialized some random value to ‘W’ and propagated forward.\n",
    ">Then, we noticed that there is some error. To reduce that error, we propagated backwards and increased the value of ‘W’.\n",
    ">After that, also we noticed that the error has increased. We came to know that, we can’t increase the ‘W’ value. \n",
    ">So, we again propagated backwards and we decreased ‘W’ value.\n",
    ">Now, we noticed that the error has reduced.\n",
    "\n",
    "So, we are trying to get the value of weight such that the error becomes minimum. Basically, we need to figure out whether we need to increase or decrease the weight value. Once we know that, we keep on updating the weight value in that direction until error becomes minimum. You might reach a point, where if you further update the weight, the error will increase. At that time you need to stop, and that is your final weight value.\n",
    "\n",
    "Q12. What are the advantages and disadvantages of neural networks?\n",
    "\n",
    "Ans: Here are some advantages of Artificial Neural Networks ( ANN)\n",
    "\n",
    "Storing information on the entire network: Information such as in traditional programming is stored on the entire network, not on a database. The disappearance of a few pieces of information in one place does not restrict the network from functioning. \n",
    "\n",
    "The ability to work with inadequate knowledge: After ANN training, the data may produce output even with incomplete information. The lack of performance here depends on the importance of the missing information. \n",
    "\n",
    "It has fault tolerance:  Corruption of one or more cells of ANN does not prevent it from generating output. This feature makes the networks fault-tolerant. \n",
    "\n",
    "Having a distributed memory: For ANN to be able to learn, it is necessary to determine the examples and to teach the network according to the desired output by showing these examples to the network. The network's progress is directly proportional to the selected instances, and if the event can not be shown to the network in all its aspects, the network can produce incorrect output \n",
    "\n",
    "Gradual corruption:  A network slows over time and undergoes relative degradation. The network problem does not immediately corrode.\n",
    "\n",
    "Ability to train machine: Artificial neural networks learn events and make decisions by commenting on similar events. \n",
    "\n",
    " Parallel processing ability:  Artificial neural networks have numerical strength that can perform more than one job at the same time. \n",
    "\n",
    "Disadvantages of Artificial Neural Networks (ANN)\n",
    "\n",
    "Hardware dependence:  Artificial neural networks require processors with parallel processing power, by their structure. For this reason, the realization of the equipment is dependent. \n",
    "\n",
    "Unexplained functioning of the network: This is the most important problem of ANN. When ANN gives a probing solution, it does not give a clue as to why and how. This reduces trust in the network. \n",
    "\n",
    "Assurance of proper network structure:  There is no specific rule for determining the structure of artificial neural networks. The appropriate network structure is achieved through experience and trial and error. \n",
    "\n",
    "The difficulty of showing the problem to the network:  ANNs can work with numerical information. Problems have to be translated into numerical values before being introduced to ANN. The display mechanism to be determined here will directly influence the performance of the network. This depends on the user's ability. \n",
    "\n",
    "The duration of the network is unknown: The network is reduced to a certain value of the error on the sample means that the training has been completed. This value does not give us optimum results. \n",
    "\n",
    "Q13. Write short notes on any two of the following:\n",
    "\n",
    "Ans:\n",
    "\n",
    "1. Biological neuron: The neuron is the fundamental building block of neural networks. In the biological systems, a neuron is a cell just like any other cell of the body, which has a DNA code and is generated in the same way as the other cells. Though it might have different DNA, the function is similar in all the organisms. A neuron comprises three major parts: the cell body (also called Soma), the dendrites, and the axon. The dendrites are like fibers branched in different directions and are connected to many cells in that cluster.\n",
    "\n",
    "Dendrites receive the signals from surrounding neurons, and the axon transmits the signal to the other neurons. At the ending terminal of the axon, the contact with the dendrite is made through a synapse. Axon is a long fiber that transports the output signal as electric impulses along its length. Each neuron has one axon. Axons pass impulses from one neuron to another like a domino effect.\n",
    "\n",
    "2. ReLU function: In the context of artificial neural networks, the rectifier or ReLU (Rectified Linear Unit) activation function, is an activation function defined as the positive part of its argument: f(x)=max(0,x). where x is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering.\n",
    "\n",
    "This activation function started showing up in the context of visual feature extraction in hierarchical neural networks starting in the late 1960s. It was later argued that it has strong biological motivations and mathematical justifications. In 2011 it was found to enable better training of deeper networks, compared to the widely used activation functions prior to 2011, e.g., the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical,counterpart, the hyperbolic tangent. The rectifier is, as of 2017, the most popular activation function for deep neural networks.\n",
    "\n",
    "Rectified linear units find applications in computer vision and speech recognition using deep neural nets and computational neuroscience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-statistics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-wilson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-phrase",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-india",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-engineer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-accident",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-heather",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-dubai",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-hybrid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-jamaica",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-subdivision",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-operation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-accounting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-judges",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-jewel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-transfer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-challenge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-jewelry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-finish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-opening",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-cycle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-coordinator",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
